<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://quocanh34.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://quocanh34.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-06-05T01:44:34+03:00</updated><id>https://quocanh34.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Captionize-it app with Pytorch and Flask</title><link href="https://quocanh34.github.io/blog/2023/image-captioning/" rel="alternate" type="text/html" title="Captionize-it app with Pytorch and Flask"/><published>2023-06-04T00:00:00+03:00</published><updated>2023-06-04T00:00:00+03:00</updated><id>https://quocanh34.github.io/blog/2023/image-captioning</id><content type="html" xml:base="https://quocanh34.github.io/blog/2023/image-captioning/"><![CDATA[<h2 id="takeaways">Takeaways</h2> <ul> <li>Understanding the encoder-decoder deep learning model.</li> <li>Understanding and utilizing training techniques.</li> <li>Deploying app using Flask.</li> <li>Basic HTML and CSS.</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="dataset">Dataset</h3> <ul> <li> <p>The <strong>Flickr8k dataset</strong> consists of a diverse collection of <strong>8,000 images</strong>, each accompanied by five different textual captions. These captions are written by human annotators to describe the content and context of the corresponding image.</p> </li> <li> <p>Link to the dataset: <strong><a href="https://www.kaggle.com/datasets/adityajn105/flickr8k">dataset link</a></strong></p> </li> </ul> <h3 id="encoder-decoder-architecture">Encoder-Decoder architecture</h3> <ul> <li> <p>This architecture has another name as <strong>sequence-to-sequence model</strong>. This model can solve variety of tasks such as machine translation, image captioning, and also generative model like cycleGAN…</p> </li> <li> <p>There are 3 main blocks in the encoder-decoder architecture: <strong>encoder</strong>, <strong>hidden vector</strong>, <strong>decoder</strong>.</p> <ul> <li>The <strong>encoder</strong> will convert the input sequence to into single-dimensional vector</li> <li>The <strong>decoder</strong> will take this vector as input, and then generate the output</li> </ul> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:515/0*bM9oRET5AGEdpaRv.png" width="50%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 1: encoder-decoder model architecture</p> </div> <h3 id="this-project">This project</h3> <ul> <li>We aim to develop an image captioning app using PyTorch and Flask by implementing an encoder-decoder model. The app will leverage a pre-trained model for the encoder component, while the decoder part will be trained from scratch. To enhance UI and accessibility, we will integrate a web interface.</li> </ul> <h2 id="dataset-and-how-to-create-vocab">Dataset and how to create vocab</h2> <ul> <li>First, in the <strong>vocab.py</strong> file, we define the <strong>Vocabulary</strong> class that handles word-to-index and index-to-word mappings.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Vocabulary<span class="o">()</span>:
    def __init__<span class="o">(</span>self, freq_threshold<span class="o">)</span>:
        self.freq_threshold <span class="o">=</span> freq_threshold
        self.itos <span class="o">=</span> <span class="o">{</span>0: <span class="s2">"&lt;PAD&gt;"</span>, 1:<span class="s2">"&lt;SOS&gt;"</span>, 2:<span class="s2">"&lt;EOS&gt;"</span>, 3:<span class="s2">"&lt;UNK&gt;"</span><span class="o">}</span>
        self.stoi <span class="o">=</span> <span class="o">{</span><span class="s2">"&lt;PAD&gt;"</span>:0, <span class="s2">"&lt;SOS&gt;"</span>:1, <span class="s2">"&lt;EOS&gt;"</span>:2, <span class="s2">"&lt;UNK&gt;"</span>:3<span class="o">}</span>
</code></pre></div></div> <ul> <li>We also use <strong>nltk.word_tokenize</strong> for tokenizing sentence.</li> <li>If a word’s frequency reaches the specified <strong>freq_threshold</strong>, it is added to the vocabulary.</li> <li>Second, To make the <strong>vocab.json</strong>, simply do like this</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vocab_file <span class="o">=</span> <span class="s2">"vocab.json"</span>
with open<span class="o">(</span>vocab_file, <span class="s2">"w"</span><span class="o">)</span> as f:
    json.dump<span class="o">(</span>vocab.stoi, f<span class="o">)</span>
</code></pre></div></div> <ul> <li>Finally, the <strong>get_loader()</strong> will return the loader and dataset</li> </ul> <h2 id="the-model-architecture">The model architecture</h2> <h3 id="inception-v3-for-the-encoder">Inception V3 for the encoder</h3> <ul> <li>An <strong>Inception Module</strong> is an image model block that aims to approximate an optimal local sparse structure in a CNN using different types of filter sizes.</li> <li>Back to previous models, we have GoogleNet as <strong>Inception V1</strong> with <strong>factorization technique</strong>, then Google introduce update <strong>Inception V2</strong> with the techniques of <strong>factorization intro symmetric concolutions</strong>. This project, we will use utilize the <strong>Inception V3</strong> with the update of <strong>promoting high dimensional representations</strong>.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*gqKM5V-uo2sMFFPDS84yJw.png" width="80%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 2: Inception V3 architecture</p> </div> <ul> <li>Using the pretrained models from Pytorch, the <strong>features vector</strong> is obtained. After, we will design a <strong>fully-connected layers</strong> to map the vector size to desired size before feeding it to the decoder</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Encoder<span class="o">(</span>nn.Module<span class="o">)</span>:
    def __init__<span class="o">(</span>self, embed_size, <span class="nv">trainEncoder</span><span class="o">=</span>False<span class="o">)</span>:
        super<span class="o">(</span>Encoder, self<span class="o">)</span>.__init__<span class="o">()</span>
        self.trainEncoder <span class="o">=</span> trainEncoder
        self.inception <span class="o">=</span> models.inception_v3<span class="o">(</span><span class="nv">weights</span><span class="o">=</span><span class="s2">"DEFAULT"</span><span class="o">)</span>
        self.inception.fc <span class="o">=</span> nn.Linear<span class="o">(</span>self.inception.fc.in_features, embed_size<span class="o">)</span>
        self.relu <span class="o">=</span> nn.ReLU<span class="o">()</span>
        self.times <span class="o">=</span> <span class="o">[]</span>
        self.dropout <span class="o">=</span> nn.Dropout<span class="o">(</span>0.5<span class="o">)</span>
</code></pre></div></div> <h3 id="lstm-for-the-decoder">LSTM for the decoder</h3> <ul> <li><strong>LSTM</strong> stands for <strong>long short-term memory networks</strong>. It is a variety of recurrent neural networks (RNNs) that are capable of learning long-term dependencies in sequence data. The problem with <strong>vanilla RNN</strong> is that it has <strong>long term dependency</strong> and <strong>gradient vanishing</strong> problems. The update of <strong>forget gate, input gate and output gate</strong> of LSTM make it an improvement over vanilla RNN because it can control the memory flow more effectively and somehow minimize the risk of gradient vanish.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://databasecamp.de/wp-content/uploads/lstm-architecture-1024x709.png" width="60%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 3: LSTM architecture</p> </div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Decoder<span class="o">(</span>nn.Module<span class="o">)</span>:
    def __init__<span class="o">(</span>self, embed_size, hidden_size, vocab_size, num_layers<span class="o">)</span>:
        super<span class="o">(</span>Decoder, self<span class="o">)</span>.__init__<span class="o">()</span>
        self.embedding <span class="o">=</span> nn.Embedding<span class="o">(</span>vocab_size, embed_size<span class="o">)</span>
        self.lstm <span class="o">=</span> nn.LSTM<span class="o">(</span>embed_size, hidden_size, num_layers<span class="o">)</span>
        self.linear <span class="o">=</span> nn.Linear<span class="o">(</span>hidden_size, vocab_size<span class="o">)</span>
        self.dropout <span class="o">=</span> nn.Dropout<span class="o">(</span>0.5<span class="o">)</span>
</code></pre></div></div> <p><strong>So questions are raised from here:</strong></p> <ul> <li>The relation of hidden state, output, and cell state in the training process?</li> <li>How we calculate the loss?</li> </ul> <p>So I will give the step-by-step in details from the features vector to the output of each time step, and how we can update the parameters.</p> <p><strong>1) Let’s derive how we feed the first vector to decoder:</strong></p> <ul> <li><strong>The features vector</strong> after going through <strong>FC layers</strong> will be concatenated with an initializing embedded <strong>START</strong> token.</li> <li><strong>The concatenated vector</strong> then will be feeded into the LSTM layer, then receive hidden_states and cell_states for each timesteps.</li> <li><strong>Output</strong> of each timestep will be calculated by feeding <strong>hidden state</strong> into linear layers to match the vocab size.</li> <li><strong>Loss</strong> will be calculated.</li> </ul> <p><strong>2) Calculate the loss using cross-entropy loss</strong></p> <ul> <li>For each timestep, we will get the <strong>output word</strong> and <strong>ground-truth word</strong>. The cross-entropy loss will be applied</li> </ul> <p><strong>3) Teacher-forcing technique</strong></p> <ul> <li>Teacher forcing is a method for quickly and efficiently training RNN models that use the <strong>ground truth</strong> from a prior time step as input.</li> <li>As we know that there will be a <strong>hallucination/mismatch</strong> between the training part and the inference part. Traditionally, the training should not use ground-truth, but here is the problem. If first timestep of decoder generate the wrong output, it will lead to the chain of wrong output, which makes the computation cost bigger.</li> <li>We can also apply the <strong>partial teacher-force</strong>. That will reduce both mismatch between training and inference and computation cost.</li> </ul> <h3 id="training-phase">Training phase</h3> <ul> <li>First, we have to specify out default parameters and transform</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Hyperparameters</span>
embed_size <span class="o">=</span> 256
hidden_size <span class="o">=</span> 256
vocab_size <span class="o">=</span> len<span class="o">(</span>dataset.vocab<span class="o">)</span>
num_layers <span class="o">=</span> 1
learning_rate <span class="o">=</span> 3e-4
num_epochs <span class="o">=</span> 150
</code></pre></div></div> <ul> <li>Second, initialize model and loss</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialize model, loss etc</span>
model <span class="o">=</span> EncoderDecoder<span class="o">(</span>embed_size, hidden_size, vocab_size, num_layers<span class="o">)</span>.to<span class="o">(</span>device<span class="o">)</span>
criterion <span class="o">=</span> nn.CrossEntropyLoss<span class="o">(</span><span class="nv">ignore_index</span><span class="o">=</span>dataset.vocab.stoi[<span class="s2">"&lt;PAD&gt;"</span><span class="o">])</span>
optimizer <span class="o">=</span> optim.Adam<span class="o">(</span>model.parameters<span class="o">()</span>, <span class="nv">lr</span><span class="o">=</span>learning_rate<span class="o">)</span>
</code></pre></div></div> <ul> <li>Lastly, the code for training with all teacher-force and cross-entropy loss above.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>epoch <span class="k">in </span>range<span class="o">(</span>num_epochs<span class="o">)</span>:
    <span class="k">for </span>idx, <span class="o">(</span>imgs, captions<span class="o">)</span> <span class="k">in </span>tqdm<span class="o">(</span>enumerate<span class="o">(</span>train_loader<span class="o">)</span>, <span class="nv">total</span><span class="o">=</span>len<span class="o">(</span>train_loader<span class="o">)</span>, <span class="nv">leave</span><span class="o">=</span>False<span class="o">)</span>:
        imgs, captions <span class="o">=</span> imgs.to<span class="o">(</span>device<span class="o">)</span>, captions.to<span class="o">(</span>device<span class="o">)</span>
        <span class="c"># Zero the gradient</span>
        optimizer.zero_grad<span class="o">()</span>
        <span class="c"># Feed forward</span>
        outputs <span class="o">=</span> model.forward<span class="o">(</span>imgs, captions[:-1]<span class="o">)</span>
        <span class="c"># Calculate batch loss</span>
        target <span class="o">=</span> captions.reshape<span class="o">(</span><span class="nt">-1</span><span class="o">)</span>
        predict <span class="o">=</span> outputs.reshape<span class="o">(</span><span class="nt">-1</span>, outputs.shape[2]<span class="o">)</span>
        loss <span class="o">=</span> criterion<span class="o">(</span>predict, target<span class="o">)</span>
        <span class="c"># Write to tensorboard</span>
        writer.add_scalar<span class="o">(</span><span class="s2">"Training loss"</span>, loss.item<span class="o">()</span>, <span class="nv">global_step</span><span class="o">=</span>step<span class="o">)</span>
        <span class="c"># Update step</span>
        step +<span class="o">=</span> 1
        <span class="c">#Eval loss</span>
        <span class="k">if </span>step % print_every <span class="o">==</span> 0:
            print<span class="o">(</span><span class="s2">"Epoch: {} loss: {:.5f}"</span>.format<span class="o">(</span>epoch,loss.item<span class="o">()))</span>
            <span class="c"># Generate the caption</span>
            model.eval<span class="o">()</span>
            print_examples<span class="o">(</span>model, device, dataset<span class="o">)</span>
            <span class="c"># Back to train mode</span>
            model.train<span class="o">()</span>
        loss.backward<span class="o">(</span>loss<span class="o">)</span>
        optimizer.step<span class="o">()</span>
        <span class="k">if </span>step % 5000 <span class="o">==</span> 0:
            save_model<span class="o">(</span>model, optimizer, step<span class="o">)</span>
</code></pre></div></div> <h2 id="the-application-of-flask-and-ui-using-html-and-css">The application of Flask and UI using HTML and CSS</h2> <h3 id="the-inference">The inference</h3> <ul> <li> <p>The <strong>inference()</strong> function will be utilized to make predicted captions of input images. It takes the path of an input image, the path of a trained model checkpoint, the target device for inference (CPU or GPU), and the path to the vocabulary JSON file.</p> </li> <li> <p>This inference() will be leveraged in the Flask app to enable users to upload images and receive corresponding captions as output.</p> </li> </ul> <h3 id="flask">Flask</h3> <ul> <li> <p>The Flask application defines the / route to handle both GET and POST requests. In the GET request, the user is presented with a simple web interface where they can upload an image. Upon submitting the image, the POST request is triggered.</p> </li> <li> <p>The application checks if the uploaded file is valid and saves it to the designated upload folder.</p> </li> <li> <p>Then, the captions, along with the image filename and path, are rendered back to the user interface for display.</p> </li> </ul> <h3 id="result-with-ui">Result with UI</h3> <div class="l-body" style="text-align:center;"> <img src="https://drive.google.com/uc?id=1T4Vy96unIIluEa4rID9JsZuDs0EzQuxw" width="100%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 4: Result</p> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>We have all went through all the steps to create an image caption app. That is absolutely interesting application of encoder-decoder architecture.</li> <li>Understanding the model architectures under the hood by coding it from scratch makes us be more active in the next development of the app.</li> <li>Understand how to use Flask, HTML and CSS to make a webapp interface</li> <li>How to improve: <ul> <li>Add cross attention</li> <li>Package to docker</li> <li>Deploy to cloud server</li> </ul> </li> </ul> <p>See you guys in the next post!</p> <h2 id="references">References</h2> <ul> <li>Image captioning paper: https://paperswithcode.com/task/image-captioning</li> <li>Flask: https://flask.palletsprojects.com/en/2.3.x/</li> <li>Encoder Decoder architecture: https://arxiv.org/abs/2110.15253</li> <li>How to use vast.ai GPU: https://vast.ai/faq</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Details of model architectures and web app]]></summary></entry><entry><title type="html">Preparing notes for NLP (word2vec)</title><link href="https://quocanh34.github.io/blog/2023/preparing-for-NLP-word2vec/" rel="alternate" type="text/html" title="Preparing notes for NLP (word2vec)"/><published>2023-03-21T00:00:00+03:00</published><updated>2023-03-21T00:00:00+03:00</updated><id>https://quocanh34.github.io/blog/2023/preparing-for-NLP-word2vec</id><content type="html" xml:base="https://quocanh34.github.io/blog/2023/preparing-for-NLP-word2vec/"><![CDATA[<h2 id="takeaways">Takeaways</h2> <ul> <li>Understanding the motivation of using neural network to create word embeddings.</li> <li>Understanding the dataset and how predicting neighboring-word model works.</li> <li>The row of updating weight matrix in the hidden layer is the word embedding corresponding to the word in the vocabulary.</li> <li>Continue reading the next post of <strong>Negative Sampling</strong> and <strong>Comparison of Skipgram and CBOW</strong>.</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="what-is-word2vec">What is Word2Vec?</h3> <ul> <li> <p>That computers can only understand the numbers( let’s say array of numbers) motivated us to create the word embeddings for all the words in the vocabulary.</p> </li> <li> <p>Other approaches for representing words we might have used in other tasks are <strong>one-hot encoding vectors</strong> or <strong>bag-of-words representations</strong>. But all of those suffer from several drawbacks that we don’t talk here (It doesn’t matter!!!).</p> </li> </ul> <h3 id="skip-gram-model">Skip-gram model</h3> <ul> <li> <p>There are two main techniques of modern word embeddings: <strong>Skip-gram</strong> and <strong>Continous bag of Words (CBOW)</strong>. But we only talk about <strong>Skip-gram</strong> here because <strong>CBOW</strong> is pretty much the same except for the goal of the model (we will talk about it later.)</p> </li> <li> <p>The <strong>Skip-gram</strong> model have expanded name which is <strong>Skip gram neural network model</strong>. To be simplified, we are going to train a simple neural network with a single hidden layer to perform such a weird task. The “weird task” we mention is that we don’t use that neural network to output something as we expect, instead we use the <strong>weights</strong> of the hidden layer to create our word embeddings.</p> </li> <li> <p>Believe me, at first I don’t believe it works but that is the interesting thing, let’s dive right into the model.</p> </li> </ul> <h2 id="the-model-architecture">The model architecture</h2> <h3 id="overview-of-the-language-model-task">Overview of the language model task</h3> <ul> <li>One of the best example of language model tasks is the next-word prediction of a smartphone keyboard.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://ugtechmag.com/wp-content/uploads/2022/03/w5J1V.png" width="50%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 1: Next word prediction task</p> </div> <ul> <li> <p>The above task is something like we take the input of 4 words <strong>I</strong>, <strong>had</strong>, <strong>such</strong>, <strong>a</strong>, then put it to the language model and the output will be all probabilities of possible words in the vocabularies (here <strong>great</strong>, <strong>great time</strong>, <strong>lovely</strong> are 3 words with highest probabilities).</p> </li> <li> <p>So the motivation for <strong>Skip-gram model</strong> is the same but reverse. So for example, we have a sentence</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                          "I had such a great time"
</code></pre></div> </div> </li> <li> <p>The <strong>Skip-gram model</strong> will take the input of the center word (let’say <strong>a</strong>) and predicting the neighboring words with the size of slicing window. The <strong>window size</strong> is how many words you want to be in the output (if window size to 2, four words <strong>had</strong>, <strong>such</strong>, <strong>great</strong>, <strong>time</strong> will be the outputs)</p> </li> <li> <p>The <strong>training sample</strong> will be like this:</p> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://media.geeksforgeeks.org/wp-content/uploads/word2vec_diagram-1.jpg" width="70%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 2: Example of training sample</p> </div> <ul> <li>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“fox”, “jumps”) than it is of (“fox”, “flies”). When the training is finished, if you give it the word “fox” as input, then it will output a much higher probability for “jumps” or “brown” than it will for “flies”.</li> </ul> <h3 id="the-skip-gram-model-in-details">The Skip-gram model in details</h3> <ul> <li> <p>First, we all know that we cannot put a word in type of string to the model. So, we have to put this in type of number, and one-hot encoding vector is the easiest way. Let’s say we have the vocabulary of 10000 unique words.</p> </li> <li> <p>So in the model below, we have to represent the input word “beautiful” as an array of 10000 dimension with 1 element value equals to 1, others will be 0. It is simply the position of the word ““beautiful”” in the vocabulary.</p> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://i0.wp.com/towardsmachinelearning.org/wp-content/uploads/2022/04/SkipGram-Model-Architecture-1.webp?resize=648%2C593&amp;ssl=1" width="70%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 3: Model architecture</p> </div> <p>So questions are raised from here:</p> <ul> <li>How about the hidden layer?</li> <li>How about the output layer?</li> <li>What is the dimension of weight matrix and also the input, hidden layer and output?</li> </ul> <p>So I will give the step-by-step in details from the input to the output through the hidden layer and how we can get the word embedding matrices from the weights matrix</p> <p>1) Let’s first define some variables:</p> <ul> <li><strong>V</strong> is the size of the vocabulary (i.e., the number of unique words in the corpus)</li> <li><strong>N</strong> is the dimensionality of the word embeddings</li> <li><strong>W</strong> is the embedding matrix of shape (V, N)</li> <li><strong>C</strong> is the context matrix of shape (N, V)</li> </ul> <p>2) The weight matrix and the output of hidden layer</p> <ul> <li>The input of word “beautiful” with dimension of (1, V)</li> <li>Then multiply with a weight matrix of (V, N), it produces the output of dimension: (1,V) * (V,N) = (1, N). This is exactly of dimension of word embedding we want.</li> <li>Let’s understand it in another way</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:1400/0*6DOQn6gxvEoix0yn.png" width="60%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 4: Example of hidden layer weight matrix</p> </div> <div class="l-body" style="text-align:center;"> <img src="https://i0.wp.com/towardsmachinelearning.org/wp-content/uploads/2022/04/image-6.webp?ssl=1" width="60%" style="margin-bottom: 12px; background-color: white;"/> <p>Figure 5: The output of hidden layer</p> </div> <ul> <li>Here we can see that each row of the weight matrix works like the word embedding for the one-hot encoding vector input, so the out put vector after doing the multiplication is already the the word embedding vector that we want.</li> </ul> <p>3) The output layer</p> <ul> <li>After getting the output of hidden layer with the dimension of (1, N), we have to multiply this output with the context matrix of dimension (N, V) to get back to the size of word in vocabulary: (1, N) . (N, V) = (1, V)</li> <li>Then we use softmax function to display the vector related to probabilities of all words in the vocab.</li> <li>Imagine that we have to calculate the loss with the true label be one-hot encoding vector like we have in the initial step.</li> <li>Then we do several steps related to calculating the gradient, update parameter, and doing the new step.</li> </ul> <p>4) The last result</p> <ul> <li>After several steps, we will get the best weight matrix. This is exactly what we want.</li> <li>Each row of this weight matrix is the word embedding for the corresponding word in the vocabulary.</li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>We have all went through all the steps to create the word embedding. That is absolutely interesting idea to using weight matrix to create word embedding.</li> <li>Understanding the motivation of algorithms under the hood will make us be more active in the field of AI and Deep Learning.</li> <li>The next part, we will talk about a method to optimize the training process of generating word embedding which is <strong>negative sampling</strong>.</li> </ul> <p>See you guys in the next post!</p> <h2 id="references">References</h2> <ul> <li>Visualize the Word2Vec: https://jalammar.github.io/illustrated-word2vec/</li> <li>Explanation of Word2Vec: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71</li> <li>Original paper: https://arxiv.org/abs/1301.3781</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Understanding of skip-gram model with formulas and image illustrations]]></summary></entry></feed>
<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="MARJu3erA7Z6jybcmR5fgSNRNtCNYMAYkk0jidGGr8A"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Captionize-it app with Pytorch and Flask | Anh Pham</title> <meta name="author" content="Anh Pham"> <meta name="description" content="Details of model architectures and web app"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://quocanh34.github.io/blog/2023/image-captioning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.highlight pre:not(.language-text){background-color:#272822;color:#f8f8f2}.highlight .hll{background-color:#272822}.highlight .comment{color:#75715e}.highlight .err{color:#960050;background-color:#1e0010}.highlight .keyword{color:#66d9ef}.highlight .l{color:#ae81ff}.highlight .n{color:#f8f8f2}.highlight .operator{color:#f92672}.highlight .punctuation{color:#f8f8f2}.highlight .cm{color:#75715e}.highlight .cp{color:#75715e}.highlight .c1{color:#75715e}.highlight .cs{color:#75715e}.highlight .ge{font-style:italic}.highlight .gs{font-weight:bold}.highlight .kc{color:#66d9ef}.highlight .kd{color:#66d9ef}.highlight .kn{color:#f92672}.highlight .kp{color:#66d9ef}.highlight .kr{color:#66d9ef}.highlight .kt{color:#66d9ef}.highlight .ld{color:#e6db74}.highlight .number{color:#ae81ff}.highlight .string{color:#e6db74}.highlight .na{color:#a6e22e}.highlight .builtin{color:#f8f8f2}.highlight .class-name{color:#a6e22e}.highlight .no{color:#66d9ef}.highlight .decorator{color:#a6e22e}.highlight .ni{color:#f8f8f2}.highlight .ne{color:#a6e22e}.highlight .function{color:#a6e22e}.highlight .nl{color:#f8f8f2}.highlight .nn{color:#f8f8f2}.highlight .nx{color:#a6e22e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f92672}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f92672}.highlight .w{color:#f8f8f2}.highlight .mf{color:#ae81ff}.highlight .mh{color:#ae81ff}.highlight .mi{color:#ae81ff}.highlight .mo{color:#ae81ff}.highlight .sb{color:#e6db74}.highlight .sc{color:#e6db74}.highlight .sd{color:#e6db74}.highlight .s2{color:#e6db74}.highlight .se{color:#ae81ff}.highlight .sh{color:#e6db74}.highlight .si{color:#e6db74}.highlight .sx{color:#e6db74}.highlight .sr{color:#e6db74}.highlight .s1{color:#e6db74}.highlight .ss{color:#e6db74}.highlight .bp{color:#f8f8f2}.highlight .vc{color:#f8f8f2}.highlight .vg{color:#f8f8f2}.highlight .vi{color:#f8f8f2}.highlight .il{color:#ae81ff}.highlight .gu{color:#75715e}.highlight .gd{color:#f92672}.highlight .gi{color:#a6e22e}</style> <script>function createButton(t,o){const n=document.querySelector("body");backToTopButton=document.createElement("span"),backToTopButton.classList.add("softr-back-to-top-button"),backToTopButton.id="softr-back-to-top-button",o?o.appendChild(backToTopButton):n.appendChild(backToTopButton),backToTopButton.style.width=t.buttonWidth,backToTopButton.style.height=t.buttonHeight,backToTopButton.style.marginRight=t.buttonDToRight,backToTopButton.style.marginBottom=t.buttonDToBottom,backToTopButton.style.borderRadius=t.roundnessSize,backToTopButton.style.boxShadow=t.shadowSize,backToTopButton.style.color=t.selectedBackgroundColor,backToTopButton.style.backgroundColor=t.selectedBackgroundColor,backToTopButton.style.position=o?"absolute":"fixed",backToTopButton.style.outline="none",backToTopButton.style.bottom="0px",backToTopButton.style.right="0px",backToTopButton.style.cursor="pointer",backToTopButton.style.textAlign="center",backToTopButton.style.border="solid 2px currentColor",backToTopButton.innerHTML='<svg class="back-to-top-button-svg" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" > <g fill="none" fill-rule="evenodd"> <path d="M0 0H32V32H0z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> <path class="back-to-top-button-img" fill-rule="nonzero" d="M11.384 13.333h9.232c.638 0 .958.68.505 1.079l-4.613 4.07c-.28.246-.736.246-1.016 0l-4.613-4.07c-.453-.399-.133-1.079.505-1.079z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> </g> </svg>',backToTopButtonSvg=document.querySelector(".back-to-top-button-svg"),backToTopButtonSvg.style.verticalAlign="middle",backToTopButtonSvg.style.margin="auto",backToTopButtonSvg.style.justifyContent="center",backToTopButtonSvg.style.width=t.svgWidth,backToTopButtonSvg.style.height=t.svgHeight,backToTopButton.appendChild(backToTopButtonSvg),backToTopButtonImg=document.querySelector(".back-to-top-button-img"),backToTopButtonImg.style.fill=t.selectedIconColor,backToTopButtonSvg.appendChild(backToTopButtonImg),backToTopButtonImg.setAttribute("d",t.buttonD),backToTopButtonImg.setAttribute("transform",t.buttonT),o||(backToTopButton.style.display="none",window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"},backToTopButton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0})}configObj={buttonD:"M8 18.568L10.8 21.333 16 16.198 21.2 21.333 24 18.568 16 10.667z",buttonT:"translate(-1148 -172) translate(832 140) translate(32 32) translate(284)",shadowSize:"none",roundnessSize:"999px",buttonDToBottom:"64px",buttonDToRight:"32px",selectedBackgroundColor:"#c2c0bf",selectedIconColor:"#a31f34",buttonWidth:"40px",buttonHeight:"40px",svgWidth:"32px",svgHeight:"32px"},document.addEventListener("DOMContentLoaded",function(){createButton(configObj,null)});</script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">AnhÂ </span>Pham</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/daily_blog/">daily</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Captionize-it app with Pytorch and Flask</h1> <p>Details of model architectures and web app</p> </d-title> <d-byline> <div class="byline grid"> <div> <h3>Published</h3> <p>June 4, 2023</p> </div> <div> <h3>Paper</h3> <p><a href="None">None</a></p> </div> <div> <h3>Code</h3> <p><a href="None">None</a></p> </div> </div> </d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#takeaways">Takeaways</a></div> <div><a href="#introduction">Introduction</a></div> <ul> <li><a href="#dataset">Dataset</a></li> <li><a href="#encoder-decoder-architecture">Encoder-Decoder architecture</a></li> <li><a href="#image-captioning-architecture">Image captioning architecture</a></li> </ul> <div><a href="#dataset">Dataset</a></div> <ul> <li><a href="#flickr8k">Flickr8k</a></li> </ul> <div><a href="#the-model-architectture">The model architectture</a></div> <ul> <li><a href="#encoder">Encoder</a></li> <li><a href="#decoder">Decoder</a></li> </ul> <div><a href="#the-training-phase">The training phase</a></div> <ul> <li><a href="#default-parameters">Default parameters</a></li> <li><a href="#utilize-vast-ai-gpu">Utilize vast.ai GPU</a></li> </ul> <div><a href="#the-application-of-flask-and-ui-using-html-and-css">The application of Flask and UI using HTML and CSS</a></div> <ul> <li><a href="#flask">Flask</a></li> <li><a href="#html-and-css">HTML and CSS</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="takeaways">Takeaways</h2> <ul> <li>Understanding the encoder-decoder deep learning model.</li> <li>Understanding and utilizing training techniques.</li> <li>Deploying app using Flask.</li> <li>Basic HTML and CSS.</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="dataset">Dataset</h3> <ul> <li> <p>The <strong>Flickr8k dataset</strong> consists of a diverse collection of <strong>8,000 images</strong>, each accompanied by five different textual captions. These captions are written by human annotators to describe the content and context of the corresponding image.</p> </li> <li> <p>Link to the dataset: <strong><a href="https://www.kaggle.com/datasets/adityajn105/flickr8k" rel="external nofollow noopener" target="_blank">dataset link</a></strong></p> </li> </ul> <h3 id="encoder-decoder-architecture">Encoder-Decoder architecture</h3> <ul> <li> <p>This architecture has another name as <strong>sequence-to-sequence model</strong>. This model can solve variety of tasks such as machine translation, image captioning, and also generative model like cycleGANâ¦</p> </li> <li> <p>There are 3 main blocks in the encoder-decoder architecture: <strong>encoder</strong>, <strong>hidden vector</strong>, <strong>decoder</strong>.</p> <ul> <li>The <strong>encoder</strong> will convert the input sequence to into single-dimensional vector</li> <li>The <strong>decoder</strong> will take this vector as input, and then generate the output</li> </ul> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:515/0*bM9oRET5AGEdpaRv.png" width="50%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 1: encoder-decoder model architecture</p> </div> <h3 id="this-project">This project</h3> <ul> <li>We aim to develop an image captioning app using PyTorch and Flask by implementing an encoder-decoder model. The app will leverage a pre-trained model for the encoder component, while the decoder part will be trained from scratch. To enhance UI and accessibility, we will integrate a web interface.</li> </ul> <h2 id="dataset-and-how-to-create-vocab">Dataset and how to create vocab</h2> <ul> <li>First, in the <strong>vocab.py</strong> file, we define the <strong>Vocabulary</strong> class that handles word-to-index and index-to-word mappings.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Vocabulary<span class="o">()</span>:
    def __init__<span class="o">(</span>self, freq_threshold<span class="o">)</span>:
        self.freq_threshold <span class="o">=</span> freq_threshold
        self.itos <span class="o">=</span> <span class="o">{</span>0: <span class="s2">"&lt;PAD&gt;"</span>, 1:<span class="s2">"&lt;SOS&gt;"</span>, 2:<span class="s2">"&lt;EOS&gt;"</span>, 3:<span class="s2">"&lt;UNK&gt;"</span><span class="o">}</span>
        self.stoi <span class="o">=</span> <span class="o">{</span><span class="s2">"&lt;PAD&gt;"</span>:0, <span class="s2">"&lt;SOS&gt;"</span>:1, <span class="s2">"&lt;EOS&gt;"</span>:2, <span class="s2">"&lt;UNK&gt;"</span>:3<span class="o">}</span>
</code></pre></div></div> <ul> <li>We also use <strong>nltk.word_tokenize</strong> for tokenizing sentence.</li> <li>If a wordâs frequency reaches the specified <strong>freq_threshold</strong>, it is added to the vocabulary.</li> <li>Second, To make the <strong>vocab.json</strong>, simply do like this</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vocab_file <span class="o">=</span> <span class="s2">"vocab.json"</span>
with open<span class="o">(</span>vocab_file, <span class="s2">"w"</span><span class="o">)</span> as f:
    json.dump<span class="o">(</span>vocab.stoi, f<span class="o">)</span>
</code></pre></div></div> <ul> <li>Finally, the <strong>get_loader()</strong> will return the loader and dataset</li> </ul> <h2 id="the-model-architecture">The model architecture</h2> <h3 id="inception-v3-for-the-encoder">Inception V3 for the encoder</h3> <ul> <li>An <strong>Inception Module</strong> is an image model block that aims to approximate an optimal local sparse structure in a CNN using different types of filter sizes.</li> <li>Back to previous models, we have GoogleNet as <strong>Inception V1</strong> with <strong>factorization technique</strong>, then Google introduce update <strong>Inception V2</strong> with the techniques of <strong>factorization intro symmetric concolutions</strong>. This project, we will use utilize the <strong>Inception V3</strong> with the update of <strong>promoting high dimensional representations</strong>.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*gqKM5V-uo2sMFFPDS84yJw.png" width="80%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 2: Inception V3 architecture</p> </div> <ul> <li>Using the pretrained models from Pytorch, the <strong>features vector</strong> is obtained. After, we will design a <strong>fully-connected layers</strong> to map the vector size to desired size before feeding it to the decoder</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Encoder<span class="o">(</span>nn.Module<span class="o">)</span>:
    def __init__<span class="o">(</span>self, embed_size, <span class="nv">trainEncoder</span><span class="o">=</span>False<span class="o">)</span>:
        super<span class="o">(</span>Encoder, self<span class="o">)</span>.__init__<span class="o">()</span>
        self.trainEncoder <span class="o">=</span> trainEncoder
        self.inception <span class="o">=</span> models.inception_v3<span class="o">(</span><span class="nv">weights</span><span class="o">=</span><span class="s2">"DEFAULT"</span><span class="o">)</span>
        self.inception.fc <span class="o">=</span> nn.Linear<span class="o">(</span>self.inception.fc.in_features, embed_size<span class="o">)</span>
        self.relu <span class="o">=</span> nn.ReLU<span class="o">()</span>
        self.times <span class="o">=</span> <span class="o">[]</span>
        self.dropout <span class="o">=</span> nn.Dropout<span class="o">(</span>0.5<span class="o">)</span>
</code></pre></div></div> <h3 id="lstm-for-the-decoder">LSTM for the decoder</h3> <ul> <li> <strong>LSTM</strong> stands for <strong>long short-term memory networks</strong>. It is a variety of recurrent neural networks (RNNs) that are capable of learning long-term dependencies in sequence data. The problem with <strong>vanilla RNN</strong> is that it has <strong>long term dependency</strong> and <strong>gradient vanishing</strong> problems. The update of <strong>forget gate, input gate and output gate</strong> of LSTM make it an improvement over vanilla RNN because it can control the memory flow more effectively and somehow minimize the risk of gradient vanish.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://databasecamp.de/wp-content/uploads/lstm-architecture-1024x709.png" width="60%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 3: LSTM architecture</p> </div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Decoder<span class="o">(</span>nn.Module<span class="o">)</span>:
    def __init__<span class="o">(</span>self, embed_size, hidden_size, vocab_size, num_layers<span class="o">)</span>:
        super<span class="o">(</span>Decoder, self<span class="o">)</span>.__init__<span class="o">()</span>
        self.embedding <span class="o">=</span> nn.Embedding<span class="o">(</span>vocab_size, embed_size<span class="o">)</span>
        self.lstm <span class="o">=</span> nn.LSTM<span class="o">(</span>embed_size, hidden_size, num_layers<span class="o">)</span>
        self.linear <span class="o">=</span> nn.Linear<span class="o">(</span>hidden_size, vocab_size<span class="o">)</span>
        self.dropout <span class="o">=</span> nn.Dropout<span class="o">(</span>0.5<span class="o">)</span>
</code></pre></div></div> <p><strong>So questions are raised from here:</strong></p> <ul> <li>The relation of hidden state, output, and cell state in the training process?</li> <li>How we calculate the loss?</li> </ul> <p>So I will give the step-by-step in details from the features vector to the output of each time step, and how we can update the parameters.</p> <p><strong>1) Letâs derive how we feed the first vector to decoder:</strong></p> <ul> <li> <strong>The features vector</strong> after going through <strong>FC layers</strong> will be concatenated with an initializing embedded <strong>START</strong> token.</li> <li> <strong>The concatenated vector</strong> then will be feeded into the LSTM layer, then receive hidden_states and cell_states for each timesteps.</li> <li> <strong>Output</strong> of each timestep will be calculated by feeding <strong>hidden state</strong> into linear layers to match the vocab size.</li> <li> <strong>Loss</strong> will be calculated.</li> </ul> <p><strong>2) Calculate the loss using cross-entropy loss</strong></p> <ul> <li>For each timestep, we will get the <strong>output word</strong> and <strong>ground-truth word</strong>. The cross-entropy loss will be applied</li> </ul> <p><strong>3) Teacher-forcing technique</strong></p> <ul> <li>Teacher forcing is a method for quickly and efficiently training RNN models that use the <strong>ground truth</strong> from a prior time step as input.</li> <li>As we know that there will be a <strong>hallucination/mismatch</strong> between the training part and the inference part. Traditionally, the training should not use ground-truth, but here is the problem. If first timestep of decoder generate the wrong output, it will lead to the chain of wrong output, which makes the computation cost bigger.</li> <li>We can also apply the <strong>partial teacher-force</strong>. That will reduce both mismatch between training and inference and computation cost.</li> </ul> <h3 id="training-phase">Training phase</h3> <ul> <li>First, we have to specify out default parameters and transform</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Hyperparameters</span>
embed_size <span class="o">=</span> 256
hidden_size <span class="o">=</span> 256
vocab_size <span class="o">=</span> len<span class="o">(</span>dataset.vocab<span class="o">)</span>
num_layers <span class="o">=</span> 1
learning_rate <span class="o">=</span> 3e-4
num_epochs <span class="o">=</span> 150
</code></pre></div></div> <ul> <li>Second, initialize model and loss</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialize model, loss etc</span>
model <span class="o">=</span> EncoderDecoder<span class="o">(</span>embed_size, hidden_size, vocab_size, num_layers<span class="o">)</span>.to<span class="o">(</span>device<span class="o">)</span>
criterion <span class="o">=</span> nn.CrossEntropyLoss<span class="o">(</span><span class="nv">ignore_index</span><span class="o">=</span>dataset.vocab.stoi[<span class="s2">"&lt;PAD&gt;"</span><span class="o">])</span>
optimizer <span class="o">=</span> optim.Adam<span class="o">(</span>model.parameters<span class="o">()</span>, <span class="nv">lr</span><span class="o">=</span>learning_rate<span class="o">)</span>
</code></pre></div></div> <ul> <li>Lastly, the code for training with all teacher-force and cross-entropy loss above.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>epoch <span class="k">in </span>range<span class="o">(</span>num_epochs<span class="o">)</span>:
    <span class="k">for </span>idx, <span class="o">(</span>imgs, captions<span class="o">)</span> <span class="k">in </span>tqdm<span class="o">(</span>enumerate<span class="o">(</span>train_loader<span class="o">)</span>, <span class="nv">total</span><span class="o">=</span>len<span class="o">(</span>train_loader<span class="o">)</span>, <span class="nv">leave</span><span class="o">=</span>False<span class="o">)</span>:
        imgs, captions <span class="o">=</span> imgs.to<span class="o">(</span>device<span class="o">)</span>, captions.to<span class="o">(</span>device<span class="o">)</span>
        <span class="c"># Zero the gradient</span>
        optimizer.zero_grad<span class="o">()</span>
        <span class="c"># Feed forward</span>
        outputs <span class="o">=</span> model.forward<span class="o">(</span>imgs, captions[:-1]<span class="o">)</span>
        <span class="c"># Calculate batch loss</span>
        target <span class="o">=</span> captions.reshape<span class="o">(</span><span class="nt">-1</span><span class="o">)</span>
        predict <span class="o">=</span> outputs.reshape<span class="o">(</span><span class="nt">-1</span>, outputs.shape[2]<span class="o">)</span>
        loss <span class="o">=</span> criterion<span class="o">(</span>predict, target<span class="o">)</span>
        <span class="c"># Write to tensorboard</span>
        writer.add_scalar<span class="o">(</span><span class="s2">"Training loss"</span>, loss.item<span class="o">()</span>, <span class="nv">global_step</span><span class="o">=</span>step<span class="o">)</span>
        <span class="c"># Update step</span>
        step +<span class="o">=</span> 1
        <span class="c">#Eval loss</span>
        <span class="k">if </span>step % print_every <span class="o">==</span> 0:
            print<span class="o">(</span><span class="s2">"Epoch: {} loss: {:.5f}"</span>.format<span class="o">(</span>epoch,loss.item<span class="o">()))</span>
            <span class="c"># Generate the caption</span>
            model.eval<span class="o">()</span>
            print_examples<span class="o">(</span>model, device, dataset<span class="o">)</span>
            <span class="c"># Back to train mode</span>
            model.train<span class="o">()</span>
        loss.backward<span class="o">(</span>loss<span class="o">)</span>
        optimizer.step<span class="o">()</span>
        <span class="k">if </span>step % 5000 <span class="o">==</span> 0:
            save_model<span class="o">(</span>model, optimizer, step<span class="o">)</span>
</code></pre></div></div> <h2 id="the-application-of-flask-and-ui-using-html-and-css">The application of Flask and UI using HTML and CSS</h2> <h3 id="the-inference">The inference</h3> <ul> <li> <p>The <strong>inference()</strong> function will be utilized to make predicted captions of input images. It takes the path of an input image, the path of a trained model checkpoint, the target device for inference (CPU or GPU), and the path to the vocabulary JSON file.</p> </li> <li> <p>This inference() will be leveraged in the Flask app to enable users to upload images and receive corresponding captions as output.</p> </li> </ul> <h3 id="flask">Flask</h3> <ul> <li> <p>The Flask application defines the / route to handle both GET and POST requests. In the GET request, the user is presented with a simple web interface where they can upload an image. Upon submitting the image, the POST request is triggered.</p> </li> <li> <p>The application checks if the uploaded file is valid and saves it to the designated upload folder.</p> </li> <li> <p>Then, the captions, along with the image filename and path, are rendered back to the user interface for display.</p> </li> </ul> <h3 id="result-with-ui">Result with UI</h3> <div class="l-body" style="text-align:center;"> <img src="https://drive.google.com/uc?id=1T4Vy96unIIluEa4rID9JsZuDs0EzQuxw" width="100%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 4: Result</p> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>We have all went through all the steps to create an image caption app. That is absolutely interesting application of encoder-decoder architecture.</li> <li>Understanding the model architectures under the hood by coding it from scratch makes us be more active in the next development of the app.</li> <li>Understand how to use Flask, HTML and CSS to make a webapp interface</li> <li>How to improve: <ul> <li>Add cross attention</li> <li>Package to docker</li> <li>Deploy to cloud server</li> </ul> </li> </ul> <p>See you guys in the next post!</p> <h2 id="references">References</h2> <ul> <li>Image captioning paper: https://paperswithcode.com/task/image-captioning</li> <li>Flask: https://flask.palletsprojects.com/en/2.3.x/</li> <li>Encoder Decoder architecture: https://arxiv.org/abs/2110.15253</li> <li>How to use vast.ai GPU: https://vast.ai/faq</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/paper-notes.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Anh Pham. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 05, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NYJ88YK0VS");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="MARJu3erA7Z6jybcmR5fgSNRNtCNYMAYkk0jidGGr8A"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Preparing notes for NLP (word2vec) | Anh Pham</title> <meta name="author" content="Anh Pham"> <meta name="description" content="Understanding of skip-gram model with formulas and image illustrations"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://quocanh34.github.io/blog/2023/preparing-for-NLP-word2vec/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.highlight pre:not(.language-text){background-color:#272822;color:#f8f8f2}.highlight .hll{background-color:#272822}.highlight .comment{color:#75715e}.highlight .err{color:#960050;background-color:#1e0010}.highlight .keyword{color:#66d9ef}.highlight .l{color:#ae81ff}.highlight .n{color:#f8f8f2}.highlight .operator{color:#f92672}.highlight .punctuation{color:#f8f8f2}.highlight .cm{color:#75715e}.highlight .cp{color:#75715e}.highlight .c1{color:#75715e}.highlight .cs{color:#75715e}.highlight .ge{font-style:italic}.highlight .gs{font-weight:bold}.highlight .kc{color:#66d9ef}.highlight .kd{color:#66d9ef}.highlight .kn{color:#f92672}.highlight .kp{color:#66d9ef}.highlight .kr{color:#66d9ef}.highlight .kt{color:#66d9ef}.highlight .ld{color:#e6db74}.highlight .number{color:#ae81ff}.highlight .string{color:#e6db74}.highlight .na{color:#a6e22e}.highlight .builtin{color:#f8f8f2}.highlight .class-name{color:#a6e22e}.highlight .no{color:#66d9ef}.highlight .decorator{color:#a6e22e}.highlight .ni{color:#f8f8f2}.highlight .ne{color:#a6e22e}.highlight .function{color:#a6e22e}.highlight .nl{color:#f8f8f2}.highlight .nn{color:#f8f8f2}.highlight .nx{color:#a6e22e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f92672}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f92672}.highlight .w{color:#f8f8f2}.highlight .mf{color:#ae81ff}.highlight .mh{color:#ae81ff}.highlight .mi{color:#ae81ff}.highlight .mo{color:#ae81ff}.highlight .sb{color:#e6db74}.highlight .sc{color:#e6db74}.highlight .sd{color:#e6db74}.highlight .s2{color:#e6db74}.highlight .se{color:#ae81ff}.highlight .sh{color:#e6db74}.highlight .si{color:#e6db74}.highlight .sx{color:#e6db74}.highlight .sr{color:#e6db74}.highlight .s1{color:#e6db74}.highlight .ss{color:#e6db74}.highlight .bp{color:#f8f8f2}.highlight .vc{color:#f8f8f2}.highlight .vg{color:#f8f8f2}.highlight .vi{color:#f8f8f2}.highlight .il{color:#ae81ff}.highlight .gu{color:#75715e}.highlight .gd{color:#f92672}.highlight .gi{color:#a6e22e}</style> <script>function createButton(t,o){const n=document.querySelector("body");backToTopButton=document.createElement("span"),backToTopButton.classList.add("softr-back-to-top-button"),backToTopButton.id="softr-back-to-top-button",o?o.appendChild(backToTopButton):n.appendChild(backToTopButton),backToTopButton.style.width=t.buttonWidth,backToTopButton.style.height=t.buttonHeight,backToTopButton.style.marginRight=t.buttonDToRight,backToTopButton.style.marginBottom=t.buttonDToBottom,backToTopButton.style.borderRadius=t.roundnessSize,backToTopButton.style.boxShadow=t.shadowSize,backToTopButton.style.color=t.selectedBackgroundColor,backToTopButton.style.backgroundColor=t.selectedBackgroundColor,backToTopButton.style.position=o?"absolute":"fixed",backToTopButton.style.outline="none",backToTopButton.style.bottom="0px",backToTopButton.style.right="0px",backToTopButton.style.cursor="pointer",backToTopButton.style.textAlign="center",backToTopButton.style.border="solid 2px currentColor",backToTopButton.innerHTML='<svg class="back-to-top-button-svg" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" > <g fill="none" fill-rule="evenodd"> <path d="M0 0H32V32H0z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> <path class="back-to-top-button-img" fill-rule="nonzero" d="M11.384 13.333h9.232c.638 0 .958.68.505 1.079l-4.613 4.07c-.28.246-.736.246-1.016 0l-4.613-4.07c-.453-.399-.133-1.079.505-1.079z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> </g> </svg>',backToTopButtonSvg=document.querySelector(".back-to-top-button-svg"),backToTopButtonSvg.style.verticalAlign="middle",backToTopButtonSvg.style.margin="auto",backToTopButtonSvg.style.justifyContent="center",backToTopButtonSvg.style.width=t.svgWidth,backToTopButtonSvg.style.height=t.svgHeight,backToTopButton.appendChild(backToTopButtonSvg),backToTopButtonImg=document.querySelector(".back-to-top-button-img"),backToTopButtonImg.style.fill=t.selectedIconColor,backToTopButtonSvg.appendChild(backToTopButtonImg),backToTopButtonImg.setAttribute("d",t.buttonD),backToTopButtonImg.setAttribute("transform",t.buttonT),o||(backToTopButton.style.display="none",window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"},backToTopButton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0})}configObj={buttonD:"M8 18.568L10.8 21.333 16 16.198 21.2 21.333 24 18.568 16 10.667z",buttonT:"translate(-1148 -172) translate(832 140) translate(32 32) translate(284)",shadowSize:"none",roundnessSize:"999px",buttonDToBottom:"64px",buttonDToRight:"32px",selectedBackgroundColor:"#c2c0bf",selectedIconColor:"#a31f34",buttonWidth:"40px",buttonHeight:"40px",svgWidth:"32px",svgHeight:"32px"},document.addEventListener("DOMContentLoaded",function(){createButton(configObj,null)});</script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Anh </span>Pham</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/daily_blog/">daily</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Preparing notes for NLP (word2vec)</h1> <p>Understanding of skip-gram model with formulas and image illustrations</p> </d-title> <d-byline> <div class="byline grid"> <div> <h3>Published</h3> <p>March 21, 2023</p> </div> <div> <h3>Paper</h3> <p><a href="None">None</a></p> </div> <div> <h3>Code</h3> <p><a href="None">None</a></p> </div> </div> </d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#takeaways">Takeaways</a></div> <div><a href="#introduction">Introduction</a></div> <ul> <li><a href="#what-is-word2vec">What is Word2Vec?</a></li> <li><a href="#skip-gram-model">Skip-gram model</a></li> </ul> <div><a href="#the-model-architectture">The model architectture</a></div> <ul> <li><a href="#the-language-model-task">The language model task</a></li> <li><a href="#the-skip-gram-model-in-details">The Skip-gram model in details</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="takeaways">Takeaways</h2> <ul> <li>Understanding the motivation of using neural network to create word embeddings.</li> <li>Understanding the dataset and how predicting neighboring-word model works.</li> <li>The row of updating weight matrix in the hidden layer is the word embedding corresponding to the word in the vocabulary.</li> <li>Continue reading the next post of <strong>Negative Sampling</strong> and <strong>Comparison of Skipgram and CBOW</strong>.</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="what-is-word2vec">What is Word2Vec?</h3> <ul> <li> <p>That computers can only understand the numbers( let’s say array of numbers) motivated us to create the word embeddings for all the words in the vocabulary.</p> </li> <li> <p>Other approaches for representing words we might have used in other tasks are <strong>one-hot encoding vectors</strong> or <strong>bag-of-words representations</strong>. But all of those suffer from several drawbacks that we don’t talk here (It doesn’t matter!!!).</p> </li> </ul> <h3 id="skip-gram-model">Skip-gram model</h3> <ul> <li> <p>There are two main techniques of modern word embeddings: <strong>Skip-gram</strong> and <strong>Continous bag of Words (CBOW)</strong>. But we only talk about <strong>Skip-gram</strong> here because <strong>CBOW</strong> is pretty much the same except for the goal of the model (we will talk about it later.)</p> </li> <li> <p>The <strong>Skip-gram</strong> model have expanded name which is <strong>Skip gram neural network model</strong>. To be simplified, we are going to train a simple neural network with a single hidden layer to perform such a weird task. The “weird task” we mention is that we don’t use that neural network to output something as we expect, instead we use the <strong>weights</strong> of the hidden layer to create our word embeddings.</p> </li> <li> <p>Believe me, at first I don’t believe it works but that is the interesting thing, let’s dive right into the model.</p> </li> </ul> <h2 id="the-model-architecture">The model architecture</h2> <h3 id="overview-of-the-language-model-task">Overview of the language model task</h3> <ul> <li>One of the best example of language model tasks is the next-word prediction of a smartphone keyboard.</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://ugtechmag.com/wp-content/uploads/2022/03/w5J1V.png" width="50%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 1: Next word prediction task</p> </div> <ul> <li> <p>The above task is something like we take the input of 4 words <strong>I</strong>, <strong>had</strong>, <strong>such</strong>, <strong>a</strong>, then put it to the language model and the output will be all probabilities of possible words in the vocabularies (here <strong>great</strong>, <strong>great time</strong>, <strong>lovely</strong> are 3 words with highest probabilities).</p> </li> <li> <p>So the motivation for <strong>Skip-gram model</strong> is the same but reverse. So for example, we have a sentence</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>                          "I had such a great time"
</code></pre></div> </div> </li> <li> <p>The <strong>Skip-gram model</strong> will take the input of the center word (let’say <strong>a</strong>) and predicting the neighboring words with the size of slicing window. The <strong>window size</strong> is how many words you want to be in the output (if window size to 2, four words <strong>had</strong>, <strong>such</strong>, <strong>great</strong>, <strong>time</strong> will be the outputs)</p> </li> <li> <p>The <strong>training sample</strong> will be like this:</p> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://media.geeksforgeeks.org/wp-content/uploads/word2vec_diagram-1.jpg" width="70%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 2: Example of training sample</p> </div> <ul> <li>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“fox”, “jumps”) than it is of (“fox”, “flies”). When the training is finished, if you give it the word “fox” as input, then it will output a much higher probability for “jumps” or “brown” than it will for “flies”.</li> </ul> <h3 id="the-skip-gram-model-in-details">The Skip-gram model in details</h3> <ul> <li> <p>First, we all know that we cannot put a word in type of string to the model. So, we have to put this in type of number, and one-hot encoding vector is the easiest way. Let’s say we have the vocabulary of 10000 unique words.</p> </li> <li> <p>So in the model below, we have to represent the input word “beautiful” as an array of 10000 dimension with 1 element value equals to 1, others will be 0. It is simply the position of the word ““beautiful”” in the vocabulary.</p> </li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://i0.wp.com/towardsmachinelearning.org/wp-content/uploads/2022/04/SkipGram-Model-Architecture-1.webp?resize=648%2C593&amp;ssl=1" width="70%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 3: Model architecture</p> </div> <p>So questions are raised from here:</p> <ul> <li>How about the hidden layer?</li> <li>How about the output layer?</li> <li>What is the dimension of weight matrix and also the input, hidden layer and output?</li> </ul> <p>So I will give the step-by-step in details from the input to the output through the hidden layer and how we can get the word embedding matrices from the weights matrix</p> <p>1) Let’s first define some variables:</p> <ul> <li> <strong>V</strong> is the size of the vocabulary (i.e., the number of unique words in the corpus)</li> <li> <strong>N</strong> is the dimensionality of the word embeddings</li> <li> <strong>W</strong> is the embedding matrix of shape (V, N)</li> <li> <strong>C</strong> is the context matrix of shape (N, V)</li> </ul> <p>2) The weight matrix and the output of hidden layer</p> <ul> <li>The input of word “beautiful” with dimension of (1, V)</li> <li>Then multiply with a weight matrix of (V, N), it produces the output of dimension: (1,V) * (V,N) = (1, N). This is exactly of dimension of word embedding we want.</li> <li>Let’s understand it in another way</li> </ul> <div class="l-body" style="text-align:center;"> <img src="https://miro.medium.com/v2/resize:fit:1400/0*6DOQn6gxvEoix0yn.png" width="60%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 4: Example of hidden layer weight matrix</p> </div> <div class="l-body" style="text-align:center;"> <img src="https://i0.wp.com/towardsmachinelearning.org/wp-content/uploads/2022/04/image-6.webp?ssl=1" width="60%" style="margin-bottom: 12px; background-color: white;"> <p>Figure 5: The output of hidden layer</p> </div> <ul> <li>Here we can see that each row of the weight matrix works like the word embedding for the one-hot encoding vector input, so the out put vector after doing the multiplication is already the the word embedding vector that we want.</li> </ul> <p>3) The output layer</p> <ul> <li>After getting the output of hidden layer with the dimension of (1, N), we have to multiply this output with the context matrix of dimension (N, V) to get back to the size of word in vocabulary: (1, N) . (N, V) = (1, V)</li> <li>Then we use softmax function to display the vector related to probabilities of all words in the vocab.</li> <li>Imagine that we have to calculate the loss with the true label be one-hot encoding vector like we have in the initial step.</li> <li>Then we do several steps related to calculating the gradient, update parameter, and doing the new step.</li> </ul> <p>4) The last result</p> <ul> <li>After several steps, we will get the best weight matrix. This is exactly what we want.</li> <li>Each row of this weight matrix is the word embedding for the corresponding word in the vocabulary.</li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>We have all went through all the steps to create the word embedding. That is absolutely interesting idea to using weight matrix to create word embedding.</li> <li>Understanding the motivation of algorithms under the hood will make us be more active in the field of AI and Deep Learning.</li> <li>The next part, we will talk about a method to optimize the training process of generating word embedding which is <strong>negative sampling</strong>.</li> </ul> <p>See you guys in the next post!</p> <h2 id="references">References</h2> <ul> <li>Visualize the Word2Vec: https://jalammar.github.io/illustrated-word2vec/</li> <li>Explanation of Word2Vec: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71</li> <li>Original paper: https://arxiv.org/abs/1301.3781</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/paper-notes.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Anh Pham. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 05, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NYJ88YK0VS");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>